{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:py36]",
      "language": "python",
      "name": "conda-env-py36-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Энтропия и выбор моделей_8/5_homework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "id": "KQHgbG8FD8gP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "uVGoL3aHD8gT",
        "colab_type": "text"
      },
      "source": [
        "# Энтропия\n",
        "\n",
        "$$H = -\\sum_i p_i \\log p_i$$\n",
        "Это магическое слово, которое все так часто употребляют и совсем не все понимают про что оно. Энтропия нам говорит (намеками) о том, насколько сложно нам судить о состоянии системы, сколько вопросов надо задать, чтобы что-то узнать. Что? Например, представим себе кубик, заполненный частичками цветной пыли, покроем кубик мысленной сеточкой из маленьких кубиков, и будем выяснять где живет самая красивая, розовая. Если бы все пылинки сидели в одном месте в уголке, то достаточно было бы одного вопроса, чтобы ее найти. Но они же разбегаются, и скорее всего придется пробежаться по всем маленьким кубикам, пока ее найдешь. Так вот, это про то, почему частицы разбегаются так как они разбегаются, провода путаются так как обычно путаются, а люди живут так как живут. Ведь вариантов все перепутать и разбросать гораздо больше, чем разложить по местам.  Наделать ошибок можно гораздо проще и разнообразней, чем не наделать. А все потому, что работает принцип максимума энтропии. А любим мы все полежать в кровати, совсем не из-за лени, а потому что работает принцип минимума потенциальной энергии, тот же который делает капельку шариком:)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "MZJJqhFED8gU",
        "colab_type": "text"
      },
      "source": [
        "### Задание 1\n",
        "Представьте, что у вас есть 5 корзин и 10 маркированных камней. Вам предложено несколько возможных распределений камней по корзинам. Посчитайте для каждого случая число всех возможных комбинаций  для реализации каждого распределения. Постройте график зависимости энтропии и логарифма числа возможных комбинаций для каждого случая  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "id": "50EagwTdD8gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy(x):\n",
        "    x  = x/np.sum(x)\n",
        "    return -np.sum([i*np.log(i) if i>0 else 0 for i in x ])\n",
        "\n",
        "d = {'A':[0, 0, 10, 0, 0], \n",
        "     'B':[0, 1, 8, 1, 0], \n",
        "     'C':[0, 2, 6, 2, 0], \n",
        "     'D':[1, 2, 4, 2, 1], \n",
        "     'E':[2, 2, 2, 2, 2]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBPwcpjGFLaa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "1861ee14-cde9-41c1-9b8b-5fb828c38979"
      },
      "source": [
        "[entropy(d[key]) for key in d.keys()]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.0,\n",
              " 0.639031859650177,\n",
              " 0.9502705392332347,\n",
              " 1.4708084763221112,\n",
              " 1.6094379124341005]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qERHs0sHFc8r",
        "colab_type": "text"
      },
      "source": [
        "Если все камни в одной корзине, энтропия минимальна, то есть равна 0. Чем они больше разбросаны, тем выше энтропия (тем больше \"хаоса\" и меньше мы знаем о системе). В последнем же случае энтропия максимальна за счёт того, что все камни разбросаны поровну, и, чтобы узнать, где какой камень, потребуется много проверок..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "cVk4PF9KD8gY",
        "colab_type": "text"
      },
      "source": [
        " ##  Линейные модели\n",
        " Или другие модели. Мы начинали с вами строить модели с предположений. Пусть вот этот параметр будет распределен нормально, и этот нормально, а этот равномерно, а этот будет описывать бета-распределением, а другой альфа (правильно нет такого, вот бета и гамма есть). А почему?\n",
        "Ну тут ответ очевиден, потому что я так сказала, а сама в книжке прочитала:) \n",
        "Но вообще за этим выбором лежит принцип максимальной энтропии. Нужно выбрать такую модель (распределение), которая может порождать максимальное разнообразие данных, при заданных ограничениях. То есть если у вас на выходе только нолики с единичками могут получатся, то нужно выбирать только то, что умеет производить нолики с единичками, во всем многообразии (но тут особо не разбежишся). \n",
        "\n",
        "Согласно принципу максимальной энтропии наиболее характерными распределениями вероятностей состояний неопределенной среды являются такие распределения, которые максимизируют выбранную меру неопределенности при заданной информации о «поведении» среды. В природе мы наблюдаем много процессов, которые имеют нормальное распределение, потому что очень многие процессы являются суммой большого количества случайных факторов (это наша заданая информация о поведении среды). А гауссовское или нормальное распределение имеет максимальную энтропию среди всех распределений с фиксированной дисперсией. Все чем оно определяется это положение центра и дисперсия. Чем больше дисперсия, тем шире распределение, тем больше энтропия. Максимума она достигает, когда распределение становится равномерным (у него уже бесконечная дисперсия).\n",
        "\n",
        "Ограничение на фиксированную дисперсию приводит к тому, что значения концентрируются в небольшой области вокруг центра, а дальше форма распределения такова, что остальные значения распределяются наиболее консервативным образом. \n",
        "\n",
        "#### Не гауссом единым\n",
        "!Но, часто у нас есть другие ограничения на данные. Например в финансах оказывается, что предположение о конечной дисперсии перестает работать и на сцену выходят так называемые распределения Леви, с \"тяжелыми хвостами\". Они позволяют описывать процессы в которых значения также концентрируются в небольшой области вокруг центра, но редкие события (большие отклонения) случаются гораздо чаще, чем предсказывают нам нормальное расределение. Как думаете почему важно учитывать эти хвосты? \n",
        "\n",
        "\n",
        "## Биномиальное распределение\n",
        "\n",
        "Наш старый знакомый. Что я пыталась сказать этими формулами? (Допишите прямо в ячейке)\n",
        "$$P(y|n,p) = \\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}$$\n",
        "\n",
        "\n",
        "$$P(y_1,y_2,y_3,.....,y_n|n,p) = p^k(1-p)^{n-k}$$\n",
        "\n",
        "\n",
        "*   Здесь по этим формулам находится вероятность получить именно такие результаты при такой-то вероятности $p$ и таком-то кол-ве $n$.\n",
        "\n",
        "Представим, что у нас есть корзина с синими и красными шарами. Мы вытаскиваем два шара, при этом у нас есть следующее ограничение, математическое ожидание вытащить синий шар за две попытки в точности единица\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "id": "5Dmgke59D8gZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {'A':[1/4, 1/4, 1/4, 1/4],\n",
        "     'B':[2/6, 1/6, 1/6, 2/6],\n",
        "     'C':[1/6, 2/6, 2/6, 1/6],\n",
        "     'D':[1/8, 4/8, 2/8, 1/8]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "mW0OW-r8D8gf",
        "colab_type": "text"
      },
      "source": [
        "##  Задание 2\n",
        "Прокомментируйте результаты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "id": "rIZjOuQzD8gf",
        "colab_type": "code",
        "outputId": "ae3ebc92-dc7b-4b56-cdd5-f9f1eab615a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "[entropy(d[key]) for key in d.keys()]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3862943611198906,\n",
              " 1.3296613488547582,\n",
              " 1.3296613488547582,\n",
              " 1.2130075659799042]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSBXfHpdIfoD",
        "colab_type": "text"
      },
      "source": [
        "И снова энтропия максимальна при равных вероятностях. При этом не важен порядок их расположения, о чём свидетельствуют энтропии 2-го и 3-го. Минимум достигается, когда вероятности распределены совсем не равномерно, как в 4-ом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "id": "Ubu2jRX5D8gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "nXh7sFfND8gp",
        "colab_type": "text"
      },
      "source": [
        "#  Обобщенные линейные модели (GLM)\n",
        "\n",
        "##  Гауссовская линейная модель\n",
        "То, что делали на предыдущих занятиях. Еще ее обычно подразумевают когда говорят про модель линейной регрессии. То есть у нас есть набор входов и выходов. А выходы представляют собой величину, которая может меняться в широких пределах и к чему то стремится (регрессия к среднему). Но среднее у нас это ни какое-то конкретное число, а само описывается линейной моделью, может даже довольно сложной, с несколькими переменными, которые могут входит нелинейно и взаимодействовать друг с другом (intercations)(!НО модель остается линейной относительно параметров )\n",
        "\n",
        "$$y_i \\sim Normal(\\mu_i, \\sigma)$$\n",
        "$$\\mu_i = \\alpha +\\beta * x_i + ...$$\n",
        "\n",
        "\n",
        "А что делать, когда наблюдаемые данные строго больше нуля или могут принимать только дискретный набор значений? Ну давайте поищем подходящие распределения, из которых могли бы быть получены наши данные (если нолики и единички - биномиальное, если произвольное целое число (всякие счетчики) - Пуассон). То есть мы поменяем правдоподобие (likelihood) на подходяшее, и у него будут уже свои параметры. А эти параметры будем описывать линейной моделью. !Но, тут опять есть проблема, линейная функция у нас работает из $R^n$ в $R$, и в случае нормального распределения нас все устраивает. А когда биномиальное, там параметр вероятность, которая может принимать значение от 0 до 1 строго. Что делать с этим? Ну математики тут быстро сообразили, нужно наложить сверху какое-нибудь отображение, которое переводит из того, что имеем в туда куда хочется. То есть будем линейной моделью описывать не интересующий нас параметр, а его, после воздействия так называемой функцией связи (link function)   \n",
        "\n",
        "$$y \\sim SomeDistr(\\theta)$$\n",
        "$$f(\\theta) = \\alpha+\\beta*x_i ..$$\n",
        "\n",
        "Тогда параметр сам получается с помощью обратной функции от функции связи\n",
        "\n",
        "$$\\theta = f^{-1}(\\alpha+\\beta*x_i)$$\n",
        "\n",
        "Теперь посмотрим на основных игроков. Чаще всего в роли функции распределении для правдоподобия будет функция из семейства экспоненциальных распределений. К ним относятся многие наши знакомые.\n",
        "\n",
        "\n",
        " - Распределение Бернулли (с фиксированным числом триалов)\n",
        " - Мультиномиальное (такого у нас не было, почти как биномиальное, но исходов больше чем два)\n",
        " - Распределение Пуассона (счетчик)\n",
        " - Нормальное распределение,\n",
        " - Экспоненциальное (про время между событиями, есть конкретное распределение, а есть семейство, не путать!)\n",
        " - Хи-квадрат\n",
        " - Гамма\n",
        " - Beta\n",
        " - Дирихле\n",
        " - Категориальное\n",
        " \n",
        " \n",
        " Посмотрим наиболее часто встречающиеся"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "5WbOVTcjD8gq",
        "colab_type": "text"
      },
      "source": [
        "##  Чаще всего встречающиеся виды обобщенных линейных моделей\n",
        "\n",
        "Про них надо уметь что-то сказать, про каждую из них. Примеры привести, распознать в задаче и оценить\n",
        "\n",
        "### Биномиальное\n",
        "\n",
        "$$y_i \\sim Binomial(n, p_i)$$\n",
        "$$logit(p_i) = \\log \\frac{p_i}{1-p_i} = \\alpha+\\beta*x_i$$\n",
        "\n",
        "$$p_i = \\frac{\\exp(\\alpha+\\beta x_i)}{1+\\exp(\\alpha+\\beta*x_i)} = \\frac{1}{1+\\exp(-(\\alpha+\\beta*x_i)} =  \\frac{1}{1+e^{-z}}  $$\n",
        "\n",
        " Ничего не напоминает? Получите последнюю формулу. \n",
        " \n",
        "###  Логарифмическая функция связи\n",
        "Тоже часто встречается, когда хотим оставаться в рамках нормального распределения, но моделировать положительный параметр, например дисперсию\n",
        "\n",
        "$$y_i \\sim Normal(\\mu, \\sigma_i)$$\n",
        "$$\\log(\\sigma_i) = \\alpha +\\beta * x_i + ...$$\n",
        "\n",
        "$$\\sigma_i = \\exp(\\alpha + \\beta*x_i)$$\n",
        "\n",
        "###  Пуассон и логарифмическая функция связи\n",
        "Моделируем редкие события, отказы, фрод, спам, катастрофы, фальсификацию на выборах \n",
        " \n",
        "$$y_i \\sim Poisson(\\lambda_i)$$\n",
        "$$\\log(\\lambda_i) = \\alpha +\\beta * x_i + ...$$\n",
        "\n",
        "$$\\sigma_i = \\exp(\\alpha + \\beta*x_i)$$\n",
        "\n",
        "###  Мультиномиальная или  softmax регрессия\n",
        "\n",
        "$$y_i \\sim Categorical(n, p[k]_i)$$\n",
        "$$score_k = \\log \\frac{p[k]_i}{p[K]_i} = \\alpha+\\beta_k*x_i$$\n",
        "\n",
        "$$p_k = \\frac{\\exp(score_k)}{\\sum_K\\exp(score_k)} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "sPZ32ki7D8gq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "id": "uwGohq-zD8gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}